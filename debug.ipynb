{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Agent2 import Agent\n",
    "from Product import Product\n",
    "from Belt import Belt\n",
    "from Buffer import Buffer\n",
    "from Pallet import Pallet\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import threading\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "class Time():\n",
    "   def __init__(self,time:int) -> None:\n",
    "      self.time = time\n",
    "   \n",
    "   def increaseTime(self):\n",
    "      self.time += 1\n",
    "\n",
    "globalTime = Time(0)\n",
    "\n",
    "numberOfEpisodes = 2500\n",
    "\n",
    "belt = Belt(1)\n",
    "buffer = Buffer(2,3)\n",
    "pallet = Pallet(3)\n",
    "agent = Agent(belt=belt,buffer=buffer,pallet=pallet, globalTime = globalTime)\n",
    "\n",
    "\n",
    "deltaTStart = 0\n",
    "deltaTEnd = 2\n",
    "\n",
    "weightStart = 1\n",
    "weightEnd = 8\n",
    "\n",
    "exitFlag = 0\n",
    "\n",
    "episode = 1\n",
    "\n",
    "with open(\"log.txt\", \"w\") as log:\n",
    "   log.write(\" ------------------ = New Trial =  -------------------\\n\")\n",
    "\n",
    "class Conveyor (threading.Thread):\n",
    "   def __init__(self, threadID, name):\n",
    "      threading.Thread.__init__(self)\n",
    "      self.threadID = threadID\n",
    "      self.name = name\n",
    "   def run(self):\n",
    "      print(\"Conveyor belt started ... \" , self.name)\n",
    "      feedProduct(self.name, 0.05)\n",
    "      print(\"Conveyor belt stopped ... \" , self.name)\n",
    "\n",
    "def feedProduct(threadName, delay):\n",
    "   while True:\n",
    "      if exitFlag:\n",
    "         return\n",
    "      time.sleep(delay)\n",
    "      if  not belt.isFull():\n",
    "         belt.addProduct(Product(arrivalTime = globalTime.time+random.randint(deltaTStart,deltaTEnd), weight = random.randint(weightStart,weightEnd)))\n",
    "\n",
    "factory = Conveyor(1, \"Belt-Thread\")\n",
    "\n",
    "factory.start()\n",
    "\n",
    "with open(\"log.csv\", \"a\") as log:\n",
    "   log.write(\"Episode,Episode Reward,Episode Steps,Normalized Episode Reward,Epsilon,Final Pallet Reward,Quartile Reward Per Step,Shipped Pallet\\n\")\n",
    "\n",
    "while episode <= numberOfEpisodes:\n",
    "   \n",
    "   time.sleep(0.05)\n",
    "   \n",
    "   if episode >= numberOfEpisodes:\n",
    "      break\n",
    "   \n",
    "   ### agent do action here (if needed!)\n",
    "   agent.learn(episode)\n",
    "   \n",
    "   torch.save(agent.act_net.state_dict(), \"./act_net_done.pth\")\n",
    "   \n",
    "   normalReward = np.nanquantile(agent.actionRewards,0.15) if not math.isnan(np.nanquantile(agent.actionRewards,0.15)) else agent.defaultActionReward\n",
    "   \n",
    "   if episode % 200 == 0:\n",
    "      print(\"-------------\")\n",
    "      print(\"Episode: \",episode)\n",
    "      print(\"Total reward: \", agent.episodeRewards[-1]/agent.episodeSteps[-1])\n",
    "      print(\"Quartile Reward: \",normalReward)\n",
    "      print(\"Final Pallet Reward: \", agent.lastRWeight)\n",
    "      print(\"----------------------\")\n",
    "   \n",
    "   with open(\"log.csv\", \"a\") as log:\n",
    "      writer = csv.writer(log, delimiter=',' , lineterminator='\\n')\n",
    "      writer.writerow([episode, agent.episodeRewards[-1], agent.episodeSteps[-1], agent.episodeRewards[-1]/agent.episodeSteps[-1], agent.epsilon,agent.lastRWeight,normalReward,agent.lastPallet])\n",
    "   \n",
    "   episode += 1\n",
    "   # belt.empty()\n",
    "   # buffer.empty()\n",
    "   # pallet.empty()\n",
    "   \n",
    "   ## tille here\n",
    "\n",
    "   ### update graphic canvas here\n",
    "   ## till here \n",
    "\n",
    "exitFlag = True\n",
    "\n",
    "\n",
    "average_rewards  = np.array(agent.episodeRewards)/np.array(agent.episodeSteps)\n",
    "plt.plot(average_rewards, \"-o\", label=\"Average Reward\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.legend('Average Reward')\n",
    "plt.savefig(\"rewards.png\", dpi=300)\n",
    "# print(\"Average Reward: \", len(average_rewards))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import get_current_fig_manager, step\n",
    "import random\n",
    "import math\n",
    "\n",
    "from numpy.core.fromnumeric import product\n",
    "from numpy.lib.function_base import copy\n",
    "\n",
    "from Product import Product\n",
    "from Belt import Belt\n",
    "from Buffer import Buffer\n",
    "from Pallet import Pallet\n",
    "from Net2 import Net\n",
    "from copy import copy as clone\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "if agent.learn_counter % agent.Q_network_evaluation == 0:\n",
    "    agent.target_net.load_state_dict(agent.act_net.state_dict())\n",
    "agent.learn_counter += 1\n",
    "\n",
    "sample_index = np.random.choice(agent.capacity, agent.batch_size)\n",
    "batch_memory = agent.memory[sample_index, :]\n",
    "batch_state = torch.FloatTensor(batch_memory[:, :agent.num_state_features])\n",
    "#note that the action must be a int\n",
    "batch_action = torch.LongTensor(batch_memory[:, agent.num_state_features:agent.num_state_features+1].astype(int))\n",
    "batch_reward = torch.FloatTensor(batch_memory[:, agent.num_state_features+1: agent.num_state_features+2])\n",
    "batch_next_state = torch.FloatTensor(batch_memory[:, -agent.num_state_features:])\n",
    "\n",
    "q_eval = agent.act_net(batch_state).gather(1, batch_action)\n",
    "q_next = agent.target_net(batch_next_state).detach()\n",
    "q_target = batch_reward + agent.gamma*q_next.max(1)[0].view(agent.batch_size, 1) - agent.target_net(batch_state).gather(1, batch_action)\n",
    "\n",
    "# loss = agent.loss(q_eval, q_target)\n",
    "# agent.optimizer.zero_grad()\n",
    "# loss.backward()\n",
    "# agent.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "sample_index = np.random.choice(agent.capacity, agent.batch_size)\n",
    "batch_memory = agent.memory[sample_index, :]\n",
    "print(len(sample_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[  0.    3.    0.    4.    0.    3.    0.    0.    0.    0.    0.    0.\n",
      "   0.    8.    0.    8.    0.    0.    0.    0.   13.  -41.6   0.    3.\n",
      "   0.    4.    0.    3.    0.    0.    0.    0.    0.    0.    0.    8.\n",
      "   0.    8.    0.    0.    0.    0. ]\n"
     ]
    }
   ],
   "source": [
    "print(agent.num_state_features)\n",
    "len(agent.memory[0,:])\n",
    "print(batch_memory[511])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1881193.1250, 1816905.8750, 1657790.7500, 1802376.5000, 1751255.1250,\n        1804798.8750, 1711546.7500, 1636652.2500, 1705764.7500, 1822496.6250,\n        1758784.5000, 1788910.1250, 1720412.5000, 1795192.5000],\n       grad_fn=<AliasBackward0>)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(agent.act_net(batch_state)[0])#.gather(0,batch_action[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_next[:].max(1)[0].view(agent.batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1773839.8750])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_eval[0]\n",
    "q_target[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3810jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}